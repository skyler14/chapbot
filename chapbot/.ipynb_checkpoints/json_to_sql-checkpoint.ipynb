{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def format_text(data):\n",
    "    return data\n",
    "\n",
    "def json_to_sql(filename):\n",
    "#     print(\"starting read of \",filename)\n",
    "    with filename.open(buffering=10000) as f:\n",
    "        for line in f: #for each json entry\n",
    "            text=line #?\n",
    "            parse=json.loads(text) #parse indiv line of text\n",
    "            row = parse\n",
    "            author = row['author'] #author is key, assign value to variable\n",
    "            body = format_text(row['body']) #TODO: define function for format_text\n",
    "            controversiality= row['controversiality']\n",
    "            created_utc= row['created_utc']\n",
    "            edited= row['edited']\n",
    "            gilded= row['gilded']\n",
    "            id_ = row['id']\n",
    "            link_id = row['link_id']\n",
    "            parent_id = row['parent_id']\n",
    "            score= row['score']\n",
    "            stickied = row['stickied']\n",
    "            subreddit = row['subreddit']\n",
    "            ups= row['ups']\n",
    "            #combine values into one list\n",
    "            params=[link_id, author, body, parent_id, subreddit, ups, score, stickied, controversiality, created_utc, edited, gilded, id_]\n",
    "#             print(params)\n",
    "            try:\n",
    "                transaction_bldr(params)\n",
    "            except Exception as e:\n",
    "                print('error: ',str(e))\n",
    "        transaction_bldr(params,batchmin=1)\n",
    "                \n",
    "def transaction_bldr(sql,batchmin=1000): #\n",
    "    sql_transaction_list.append(sql)\n",
    "    if len(sql_transaction_list) > batchmin:\n",
    "        try: #executes raw SQL code\n",
    "            c.executemany(\"\"\"INSERT INTO reddit_comment_table (link_id, author, body, parent_id, subreddit, ups, score, stickied, controversiality, created_utc, edited, gilded, id_) \\\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\", sql_transaction_list) #'?' for each varaialbe input\n",
    "        except Exception as e:\n",
    "            print('error', str(e))\n",
    "        cxn.commit()   \n",
    "        sql_transaction_list.clear()\n",
    "# def transaction_bldr(sql):\n",
    "#     global sql_transaction\n",
    "#     sql_transaction.append(sql)\n",
    "#     if len(sql_transaction) > 1000:\n",
    "#         c.execute('BEGIN TRANSACTION')\n",
    "#         for s in sql_transaction:\n",
    "#             try:\n",
    "#                 c.execute(s)\n",
    "#             except:\n",
    "#                 pass\n",
    "#         connection.commit()\n",
    "#         sql_transaction = []\n",
    "        \n",
    "\n",
    "\n",
    "def create_table():\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS reddit_comment_table (link_id TEXT PRIMARY_KEY,\n",
    "id_ TEXT,\n",
    "parent_id TEXT,\n",
    "author TEXT,\n",
    "body TEXT,\n",
    "subreddit TEXT,\n",
    "controversiality INT, \n",
    "created_utc INT,\n",
    "edited TEXT,\n",
    "gilded INT,\n",
    "score INT,\n",
    "stickied \n",
    "subreddit TEXT,\n",
    "ups INT) \"\"\")\n",
    "    cxn.commit()\n",
    "    \n",
    "def dir_crawler(): \n",
    "    imglist=[]\n",
    "    for paths, dirs, files in os.walk(TRAIN_IMAGE_DIR): #finds reddit data\n",
    "        # print(dirs)\n",
    "        for f in files: #for each file\n",
    "            filename = os.path.join(paths,f)\n",
    "            print(filename) #replace w/ desired task\n",
    "            # print(os.path.dirname(filename))\n",
    "            # path = os.path.dirname(filename)\n",
    "            # os.chdir(path))\n",
    "\n",
    "    \n",
    "# program start\n",
    "\n",
    "\n",
    "cwd = Path(os.getcwd())\n",
    "filedir=cwd.parents[0] / \"reddit\" / \"2005\" / \"RC_2005-12\"\n",
    "\n",
    "file=Path(filedir)\n",
    "\n",
    "parsed=[]\n",
    "parse=\"\"\n",
    "\n",
    "timeframe='2005-12'\n",
    "sql_transaction_list=[]\n",
    "cxn=sqlite3.connect('{}.db'.format(timeframe))\n",
    "c = cxn.cursor()\n",
    "    \n",
    "create_table() \n",
    "\n",
    "json_to_sql(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hannah\\Desktop\\UT Austin\\Analytics - MSBA\\Fall 19\\Data mangement intro\\Final project\\chaptbot2\\chapbot\\reddit\\2005\\RC_2005-12\n"
     ]
    }
   ],
   "source": [
    "# if you want to do it for multiple dirs here just specify the folder in filedir\n",
    "# and in the for loop add the json_to_sql function\n",
    "\n",
    "cwd = Path(os.getcwd())\n",
    "filedir=cwd.parents[0] / \"reddit\"  \n",
    "\n",
    "def dir_crawler(folder):\n",
    "    for paths, dirs, files in os.walk(str(folder)):\n",
    "        # print(dirs)\n",
    "        for f in files:\n",
    "            filename = Path(os.path.join(paths,f))\n",
    "#             add file filter logic here to split your data as you see fit, examples of filters I needed\n",
    "            if not f.endswith(('.bz2','-checkpoint','.Rhistory')):\n",
    "                yield filename\n",
    "            # print(os.path.dirname(filename))\n",
    "            # path = os.path.dirname(filename)\n",
    "            # os.chdir(path))\n",
    "\n",
    "for i in dir_crawler(filedir):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for reference with regards to directory walking\n",
    "# quake_df = pd.DataFrame(columns=['quakename', 'sensor_id', 'max_sensor'])\n",
    "# #print(quake_df)\n",
    "\n",
    "# for paths, dir, files in os.walk('/home/skyler/Downloads/data/csn'):\n",
    "#     #print(dirnames)\n",
    "#     for f in files:\n",
    "#         #print(f)\n",
    "#         filename = os.path.join(paths,f)\n",
    "#         #print(os.path.basename(os.path.dirname(filename)))\n",
    "#         #print(os.path.dirname(filename))\n",
    "#         path = os.path.dirname(filename)\n",
    "#         os.chdir(path)\n",
    "# #       read here\n",
    "\n",
    "    \n",
    "    \n",
    "# #       st = read(f, debug_headers=True)\n",
    "# #       print(max(st[0].data))\n",
    "# #       quake_df=quake_df.append({'quakename': os.path.basename(os.path.dirname(filename)), 'sensor_id': f, 'max_sensor': max(st[0].data)-np.median(st[0].data), 'max_clock': np.argmax(st[0].data),'min_sensor': min(st[0].data,)-np.median(st[0].data), 'min_clock': np.argmin(st[0].data),'Latitude':st[0].stats.sac['stla'],'Longitude':st[0].stats.sac['stlo']}, ignore_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"author\": \"sempf\",\n",
      "    \"author_flair_css_class\": null,\n",
      "    \"author_flair_text\": null,\n",
      "    \"body\": \"Again.  Relax.  It's funny.  Sheesh.\",\n",
      "    \"controversiality\": 0,\n",
      "    \"created_utc\": 1136073220,\n",
      "    \"distinguished\": null,\n",
      "    \"edited\": false,\n",
      "    \"gilded\": 0,\n",
      "    \"id\": \"c2713\",\n",
      "    \"link_id\": \"t3_22542\",\n",
      "    \"parent_id\": \"t1_c2701\",\n",
      "    \"retrieved_on\": 1473821517,\n",
      "    \"score\": 9,\n",
      "    \"stickied\": false,\n",
      "    \"subreddit\": \"reddit.com\",\n",
      "    \"subreddit_id\": \"t5_6\",\n",
      "    \"ups\": 9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# !pip install jupyterlab_sql\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cwd = Path(os.getcwd())\n",
    "filedir=cwd.parents[0] / \"reddit\" / \"2005\" / \"RC_2005-12\"\n",
    "\n",
    "file=Path(filedir)\n",
    "\n",
    "parsed=[]\n",
    "parse=\"\"\n",
    "\n",
    "with file.open() as f:\n",
    "    for line in f:\n",
    "        text=line\n",
    "        parsed.append(json.loads(text))\n",
    "        parse=json.loads(text)\n",
    "#         print(json.dumps(parse, indent=4, sort_keys=True))\n",
    "# print(parsed)\n",
    "print(json.dumps(parse, indent=4, sort_keys=True))\n",
    "\n",
    "timeframe='2005-12'\n",
    "sql_transaction=[]\n",
    "cxn=sqlite3.connect('{}.db'.format(timeframe)) #connect to a file to be named smthing.db\n",
    "c = cxn.cursor()\n",
    "\n",
    "\n",
    "# creates table if it does not exist\n",
    "def create_table():\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS reddit_comment_table (link_id TEXT PRIMARY_KEY,\n",
    "id_ TEXT,\n",
    "parent_id TEXT,\n",
    "author TEXT,\n",
    "body TEXT,\n",
    "subreddit TEXT,\n",
    "controversiality INT, \n",
    "created_utc INT,\n",
    "edited TEXT,\n",
    "gilded INT,\n",
    "score INT,\n",
    "stickied \n",
    "subreddit TEXT,\n",
    "ups INT) \"\"\")\n",
    "    cxn.commit()\n",
    "\n",
    "create_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t3_22542', 'sempf', \"Again.  Relax.  It's funny.  Sheesh.\", 't1_c2701', 'reddit.com', 9, 0, 1136073220, False, 0, 'c2713', 9, False]\n"
     ]
    }
   ],
   "source": [
    "# sample operations to clean up and prep data for SQL submission \n",
    "\n",
    "# where we clean up the body text\n",
    "# TODO: expand it from STUB function\n",
    "\n",
    "\n",
    "\n",
    "row = parse\n",
    "author = row['author']\n",
    "body = format_text(row['body'])\n",
    "controversiality= row['controversiality']\n",
    "created_utc= row['created_utc']\n",
    "edited= row['edited']\n",
    "gilded= row['gilded']\n",
    "id_ = row['id']\n",
    "link_id = row['link_id']\n",
    "parent_id = row['parent_id']\n",
    "score= row['score']\n",
    "stickied = row['stickied']\n",
    "subreddit = row['subreddit']\n",
    "ups= row['ups']\n",
    "params=[link_id, author, body, parent_id, subreddit, ups, controversiality, created_utc, edited, gilded, id_, score, stickied]\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

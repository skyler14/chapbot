{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hannah\\Anaconda3\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "# Using THREE month's worth of world news posts\n",
    "'''12-1-2019 EDA'''\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer # CountVectorizer converts a collection of text documents to a matrix of token counts\n",
    "import warnings\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA # Load LDA model from SKL\n",
    "from collections import Counter\n",
    "import pyLDAvis # For interactive topic model visualization\n",
    "from pyLDAvis import sklearn as sklearn_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Hannah\\\\Desktop\\\\UT_Austin\\\\Analytics_MSBA\\\\Fall_19\\\\Data_mgmt_intro\\\\Final_project\\\\summer19_worldnews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1557015514</td>\n",
       "      <td>t5_2qh13</td>\n",
       "      <td>t3_b8k17q</td>\n",
       "      <td>t1_emi96x1</td>\n",
       "      <td>0</td>\n",
       "      <td>1561539494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>emj0itm</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add Australia and Canada to the trump ally lis...</td>\n",
       "      <td>Houjix</td>\n",
       "      <td>1558528774</td>\n",
       "      <td>t5_2qh13</td>\n",
       "      <td>t3_b9pc0e</td>\n",
       "      <td>t1_ek629yu</td>\n",
       "      <td>0</td>\n",
       "      <td>1563090093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eofaisv</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So, I just noticed your reply, and it worries ...</td>\n",
       "      <td>epistemic_zoop</td>\n",
       "      <td>1560687132</td>\n",
       "      <td>t5_2qh13</td>\n",
       "      <td>t3_b9wu6k</td>\n",
       "      <td>t1_ek90vw5</td>\n",
       "      <td>0</td>\n",
       "      <td>1570082531</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>erbmu65</td>\n",
       "      <td>worldnews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body          author  \\\n",
       "0                                          [removed]       [deleted]   \n",
       "1  Add Australia and Canada to the trump ally lis...          Houjix   \n",
       "2  So, I just noticed your reply, and it worries ...  epistemic_zoop   \n",
       "\n",
       "   created_utc subreddit_id    link_id   parent_id  score  retrieved_on  \\\n",
       "0   1557015514     t5_2qh13  t3_b8k17q  t1_emi96x1      0    1561539494   \n",
       "1   1558528774     t5_2qh13  t3_b9pc0e  t1_ek629yu      0    1563090093   \n",
       "2   1560687132     t5_2qh13  t3_b9wu6k  t1_ek90vw5      0    1570082531   \n",
       "\n",
       "   controversiality  gilded       id  subreddit  \n",
       "0                 0       0  emj0itm  worldnews  \n",
       "1                 0       0  eofaisv  worldnews  \n",
       "2                 0       0  erbmu65  worldnews  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "\n",
    "# Remove null comments\n",
    "df = df[pd.notnull(df['body'])]\n",
    "\n",
    "# Convert all posts to string\n",
    "df['body'] = df['body'].astype(str)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\w\n",
      "<ipython-input-6-26edbc285c0a>:7: DeprecationWarning: invalid escape sequence \\w\n",
      "  df['body_sw_p'] = df['body_sw'].str.replace('[^\\w\\s]', '').str.lower()\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "\n",
    "# Remove stopwords\n",
    "df['body_sw'] = df['body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# Remove special characters, make all characters lowercase\n",
    "df['body_sw_p'] = df['body_sw'].str.replace('[^\\w\\s]', '').str.lower()\n",
    "\n",
    "# Remove other common words\n",
    "mask = df['body_sw_p'].str.contains('deleted','people')\n",
    "df=df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed titles to get matrix of token counts\n",
    "count_data_tokens = count_vectorizer.fit_transform(df['body_sw_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Function to print topics and the top 10 labels in each\n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names() #words in corpus\n",
    "    for topic_idx, topic in enumerate(model.components_): #enumerate adds a counter to an iterable\n",
    "        print(\"\\nTopic #%d:\" % topic_idx) #%d is used as a placeholder for numeric or decimal values\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])) #numpy.argsort() returns the indices that would sort an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "companies company business market like money make capitalism buy free\n",
      "\n",
      "Topic #1:\n",
      "climate change energy oil carbon power global tax emissions nuclear\n",
      "\n",
      "Topic #2:\n",
      "removed trump report evidence russian mueller president investigation russia said\n",
      "\n",
      "Topic #3:\n",
      "people problem money need want like think stop government make\n",
      "\n",
      "Topic #4:\n",
      "china world countries country russia chinese india population europe japan\n",
      "\n",
      "Topic #5:\n",
      "people good work america job pay rich american americans money\n",
      "\n",
      "Topic #6:\n",
      "iran war military nuclear weapons trump attack iranian sanctions deal\n",
      "\n",
      "Topic #7:\n",
      "water food like use thanks plastic flag used big cancer\n",
      "\n",
      "Topic #8:\n",
      "israel land earth humans human water palestinians like israeli years\n",
      "\n",
      "Topic #9:\n",
      "trump hes vote president election party like obama republicans voting\n",
      "\n",
      "Topic #10:\n",
      "people point think thats im like make say things argument\n",
      "\n",
      "Topic #11:\n",
      "years year like people time ago old day million 10\n",
      "\n",
      "Topic #12:\n",
      "right uk left eu people party wing brexit political far\n",
      "\n",
      "Topic #13:\n",
      "reddit account automatically questions contact action moderators concerns bot rworldnews\n",
      "\n",
      "Topic #14:\n",
      "article read news comment source like media link sources sounds\n",
      "\n",
      "Topic #15:\n",
      "people country government war saudi camps like power world countries\n",
      "\n",
      "Topic #16:\n",
      "gt said version welcome best far comments im make tldr\n",
      "\n",
      "Topic #17:\n",
      "law police crime legal illegal hong laws kong court rape\n",
      "\n",
      "Topic #18:\n",
      "people like think bad youre really thats im know hate\n",
      "\n",
      "Topic #19:\n",
      "im shit know dont fuck lol thats like sure fucking\n"
     ]
    }
   ],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 20\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "# n_jobs specifyies the maximum number of concurrently running jobs. If set to -1, all CPUs are used. \n",
    "# n_components number of features which a transformer should transform the input into\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1, random_state=47)\n",
    "lda.fit(count_data_tokens)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hannah\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "# # Support visual to see the separation between topic components and the label frequencies for each.\n",
    "# LDAvis_prepped = sklearn_lda.prepare(lda, count_data_tokens, count_vectorizer)\n",
    "# pyLDAvis.display(LDAvis_prepped)\n",
    "# pyLDAvis.save_html(LDAvis_prepped, './LDAvis_prepped'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Topic</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.698161</td>\n",
       "      <td>0.014362</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.029355</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>0.119631</td>\n",
       "      <td>0.016256</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.010259</td>\n",
       "      <td>0.04982</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.02057</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100f</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.987250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100k</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960664</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.018240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101httpwwwredditcomrhelpcomments2bx3cjreddit_101</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10k</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.977707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Topic                                                   1         2   \\\n",
       "Term                                                                   \n",
       "10                                                0.001683  0.000579   \n",
       "100f                                                   NaN       NaN   \n",
       "100k                                                   NaN       NaN   \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN       NaN   \n",
       "10k                                                    NaN       NaN   \n",
       "\n",
       "Topic                                                   3         4   \\\n",
       "Term                                                                   \n",
       "10                                                0.004998  0.002315   \n",
       "100f                                                   NaN       NaN   \n",
       "100k                                                   NaN       NaN   \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN       NaN   \n",
       "10k                                                    NaN       NaN   \n",
       "\n",
       "Topic                                                   5         6   \\\n",
       "Term                                                                   \n",
       "10                                                0.698161  0.014362   \n",
       "100f                                              0.987250       NaN   \n",
       "100k                                              0.960664  0.001013   \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN       NaN   \n",
       "10k                                               0.977707       NaN   \n",
       "\n",
       "Topic                                                   7         8   \\\n",
       "Term                                                                   \n",
       "10                                                0.002788  0.029355   \n",
       "100f                                                   NaN       NaN   \n",
       "100k                                              0.006080  0.018240   \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN       NaN   \n",
       "10k                                                    NaN       NaN   \n",
       "\n",
       "Topic                                                   9         10  \\\n",
       "Term                                                                   \n",
       "10                                                0.002104  0.012415   \n",
       "100f                                                   NaN       NaN   \n",
       "100k                                                   NaN       NaN   \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN       NaN   \n",
       "10k                                                    NaN       NaN   \n",
       "\n",
       "Topic                                                   11        12  \\\n",
       "Term                                                                   \n",
       "10                                                0.119631  0.016256   \n",
       "100f                                                   NaN       NaN   \n",
       "100k                                              0.001013  0.004053   \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN       NaN   \n",
       "10k                                               0.006837  0.005128   \n",
       "\n",
       "Topic                                                   13        14       15  \\\n",
       "Term                                                                            \n",
       "10                                                0.001315  0.010259  0.04982   \n",
       "100f                                                   NaN       NaN      NaN   \n",
       "100k                                              0.008107       NaN      NaN   \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN       NaN      NaN   \n",
       "10k                                               0.005128       NaN      NaN   \n",
       "\n",
       "Topic                                                   16        17       18  \\\n",
       "Term                                                                            \n",
       "10                                                0.000684  0.000737  0.02057   \n",
       "100f                                                   NaN       NaN      NaN   \n",
       "100k                                                   NaN       NaN      NaN   \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN       NaN      NaN   \n",
       "10k                                                    NaN       NaN      NaN   \n",
       "\n",
       "Topic                                                   19        20  \n",
       "Term                                                                  \n",
       "10                                                0.011889       NaN  \n",
       "100f                                                   NaN       NaN  \n",
       "100k                                                   NaN       NaN  \n",
       "101httpwwwredditcomrhelpcomments2bx3cjreddit_101       NaN  0.999914  \n",
       "10k                                               0.003419       NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # (i) A file showing which words load on which topics\n",
    "\n",
    "# temp = pd.DataFrame(LDAvis_prepped[2])\n",
    "# temp = temp.reset_index()\n",
    "# temp = temp\n",
    "# #.rename(columns={4: 'Wildlife', 2: 'Water', 3: 'Photography', 1: 'Landscape', 5: 'Culture'})\n",
    "# pd.crosstab(temp['Term'],temp['Topic'],values=temp['Freq'],aggfunc=sum).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2292720"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our earliest analysis code that was used to make an SQLite database with a subset of the data\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# stub to format text\n",
    "def format_text(data):\n",
    "    return data\n",
    "\n",
    "# this takes each column and row in a buffer input of 10000 lines and creates a parameterized \n",
    "def json_to_sql(filename):\n",
    "#     print(\"starting read of \",filename)\n",
    "    with filename.open(buffering=10000) as f:\n",
    "        for line in f: #for each json entry\n",
    "            text=line #?\n",
    "            parse=json.loads(text) #parse indiv line of text\n",
    "            row = parse\n",
    "            author = row['author'] #author is key, assign value to variable\n",
    "            body = format_text(row['body']) #TODO: define function for format_text\n",
    "            controversiality= row['controversiality']\n",
    "            created_utc= row['created_utc']\n",
    "            edited= row['edited']\n",
    "            gilded= row['gilded']\n",
    "            id_ = row['id']\n",
    "            link_id = row['link_id']\n",
    "            parent_id = row['parent_id']\n",
    "            score= row['score']\n",
    "            subreddit = row['subreddit']\n",
    "            ups= row['ups']\n",
    "            #combine values into one list\n",
    "            params=[link_id, author, body, parent_id, subreddit, ups, score, controversiality, created_utc, edited, gilded, id_]\n",
    "#             print(params)\n",
    "            try:\n",
    "                transaction_bldr(params)\n",
    "            except Exception as e:\n",
    "                print('error: ',str(e))\n",
    "        transaction_bldr(params,batchmin=1)\n",
    "                \n",
    "def transaction_bldr(sql,batchmin=1000): #\n",
    "    sql_transaction_list.append(sql)\n",
    "    if len(sql_transaction_list) > batchmin:\n",
    "        try: #executes raw SQL code\n",
    "            c.executemany(\"\"\"INSERT INTO reddit_comment_table (link_id, author, body, parent_id, subreddit, ups, score, controversiality, created_utc, edited, gilded, id_) \\\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\", sql_transaction_list) #'?' for each varaialbe input\n",
    "        except Exception as e:\n",
    "            print('error', str(e))\n",
    "        cxn.commit()   \n",
    "        sql_transaction_list.clear()\n",
    "# def transaction_bldr(sql):\n",
    "#     global sql_transaction\n",
    "#     sql_transaction.append(sql)\n",
    "#     if len(sql_transaction) > 1000:\n",
    "#         c.execute('BEGIN TRANSACTION')\n",
    "#         for s in sql_transaction:\n",
    "#             try:\n",
    "#                 c.execute(s)\n",
    "#             except:\n",
    "#                 pass\n",
    "#         connection.commit()\n",
    "#         sql_transaction = []\n",
    "        \n",
    "\n",
    "\n",
    "def create_table():\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS reddit_comment_table (link_id TEXT PRIMARY_KEY,\n",
    "id_ TEXT,\n",
    "parent_id TEXT,\n",
    "author TEXT,\n",
    "body TEXT,\n",
    "subreddit TEXT,\n",
    "controversiality INT, \n",
    "created_utc INT,\n",
    "edited TEXT,\n",
    "gilded INT,\n",
    "score INT,\n",
    "subreddit TEXT,\n",
    "ups INT) \"\"\")\n",
    "    cxn.commit()\n",
    "    \n",
    "def dir_crawler(): \n",
    "    imglist=[]\n",
    "    for paths, dirs, files in os.walk(TRAIN_IMAGE_DIR): #finds reddit data\n",
    "        # print(dirs)\n",
    "        for f in files: #for each file\n",
    "            filename = os.path.join(paths,f)\n",
    "            print(filename) #replace w/ desired task\n",
    "            # print(os.path.dirname(filename))\n",
    "            # path = os.path.dirname(filename)\n",
    "            # os.chdir(path))\n",
    "\n",
    "    \n",
    "# heres just for the first reddit file, so you just have to extract 1 file\n",
    "# we did this for 2005 to 2008 initially but worldnews is especially a case where data staleness matters\n",
    "\n",
    "cwd = Path(os.getcwd())\n",
    "filedir=cwd.parents[0] / \"reddit\" / \"2005\" / \"RC_2005-12\"\n",
    "\n",
    "file=Path(filedir)\n",
    "\n",
    "parsed=[]\n",
    "parse=\"\"\n",
    "\n",
    "timeframe='2005-12'\n",
    "sql_transaction_list=[]\n",
    "cxn=sqlite3.connect('{}.db'.format(timeframe))\n",
    "c = cxn.cursor()\n",
    "    \n",
    "create_table() \n",
    "\n",
    "json_to_sql(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to do it for multiple dirs here just specify the folder in filedir\n",
    "# and in the for loop add the json_to_sql function\n",
    "# old reddit data was organized in a parent dir adjacent to the github repo folder\n",
    "cwd = Path(os.getcwd())\n",
    "filedir=cwd.parents[0] / \"reddit\"  \n",
    "\n",
    "def main(filedir):\n",
    "    file=Path(filedir)\n",
    "\n",
    "    parsed=[]\n",
    "    parse=\"\"\n",
    "    sql_transaction_list=[]\n",
    "    cxn=sqlite3.connect('{}.db'.format(\"Reddit_table\"))\n",
    "    c = cxn.cursor()\n",
    "\n",
    "    create_table() \n",
    "\n",
    "    json_to_sql(file)\n",
    "\n",
    "def dir_crawler(folder):\n",
    "    for paths, dirs, files in os.walk(str(folder)):\n",
    "        # print(dirs)\n",
    "        for f in files:\n",
    "            filename = Path(os.path.join(paths,f))\n",
    "#             add file filter logic here to split your data as you see fit, examples of filters I needed\n",
    "            if not f.endswith(('.bz2','-checkpoint','.Rhistory')):\n",
    "                yield filename\n",
    "            # print(os.path.dirname(filename))\n",
    "            # path = os.path.dirname(filename)\n",
    "            # os.chdir(path))\n",
    "\n",
    "for i in dir_crawler(filedir):\n",
    "#     print(Path(i))\n",
    "    main(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for reference with regards to directory walking\n",
    "# quake_df = pd.DataFrame(columns=['quakename', 'sensor_id', 'max_sensor'])\n",
    "# #print(quake_df)\n",
    "\n",
    "# for paths, dir, files in os.walk('/home/skyler/Downloads/data/csn'):\n",
    "#     #print(dirnames)\n",
    "#     for f in files:\n",
    "#         #print(f)\n",
    "#         filename = os.path.join(paths,f)\n",
    "#         #print(os.path.basename(os.path.dirname(filename)))\n",
    "#         #print(os.path.dirname(filename))\n",
    "#         path = os.path.dirname(filename)\n",
    "#         os.chdir(path)\n",
    "# #       read here\n",
    "\n",
    "    \n",
    "    \n",
    "# #       st = read(f, debug_headers=True)\n",
    "# #       print(max(st[0].data))\n",
    "# #       quake_df=quake_df.append({'quakename': os.path.basename(os.path.dirname(filename)), 'sensor_id': f, 'max_sensor': max(st[0].data)-np.median(st[0].data), 'max_clock': np.argmax(st[0].data),'min_sensor': min(st[0].data,)-np.median(st[0].data), 'min_clock': np.argmin(st[0].data),'Latitude':st[0].stats.sac['stla'],'Longitude':st[0].stats.sac['stlo']}, ignore_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # !pip install jupyterlab_sql\n",
    "# import json\n",
    "# import sqlite3\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# cwd = Path(os.getcwd())\n",
    "# filedir=cwd.parents[0] / \"reddit\" / \"2005\" / \"RC_2005-12\"\n",
    "\n",
    "# file=Path(filedir)\n",
    "\n",
    "# parsed=[]\n",
    "# parse=\"\"\n",
    "\n",
    "# with file.open() as f:\n",
    "#     for line in f:\n",
    "#         text=line\n",
    "#         parsed.append(json.loads(text))\n",
    "#         parse=json.loads(text)\n",
    "# #         print(json.dumps(parse, indent=4, sort_keys=True))\n",
    "# #         print(parsed)\n",
    "# print(json.dumps(parse, indent=4, sort_keys=True))\n",
    "\n",
    "# timeframe='2005-12'\n",
    "# sql_transaction=[]\n",
    "# cxn=sqlite3.connect('{}.db'.format(timeframe)) #connect to a file to be named smthing.db\n",
    "# c = cxn.cursor()\n",
    "    \n",
    "    \n",
    "# create_table()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
